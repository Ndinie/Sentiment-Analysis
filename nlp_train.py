# -*- coding: utf-8 -*-
"""NLP_train.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Q779L9xcuQ1IPv_uyKULVFSr3WSdkZ2G
"""

import os
import re
import datetime
import numpy as np
import pandas as pd

from tensorflow.keras.layers import Embedding
from tensorflow.keras.utils import plot_model
from tensorflow.keras import Input, Sequential
from sklearn.preprocessing import OneHotEncoder
from tensorflow.keras.callbacks import TensorBoard
from sklearn.model_selection import train_test_split
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.layers import LSTM, Dense, Dropout, Bidirectional

"""# STEP 1) DATA LOADING"""

nlp_df = pd.read_csv('https://raw.githubusercontent.com/Ankit152/IMDB-sentiment-analysis/master/IMDB-Dataset.csv')

"""# STEP 2) DATA INSPECTION"""

nlp_df.info()

nlp_df.describe().T

nlp_df.head()

nlp_df.duplicated().sum()

nlp_df.isna().sum()

"""# STEP 3) DATA CLEANING

symbols and HTML tags have to be removed
"""

test = nlp_df['review'][20]

print('______________________________BEFORE____________________________________')
test

print('______________________________AFTER____________________________________')
re.sub('<.*?>','',test)

re.sub('[^a-zA-Z]',' ',test).lower()

test = re.sub('<.*?>','',test)
test = re.sub('[^a-zA-Z]',' ',test).lower().split()
test

review = nlp_df['review']
sentiment = nlp_df['sentiment']

for index, text in enumerate(review):
  # to remove html tags
  # anything within the <> will be removed including <>
  # ? to tell re dont be greedy so it wont capture anything
  # from the first < to the last > in the document
  review[index] = re.sub('<.*?>','',text)
  review[index] = re.sub('[^a-zA-Z]',' ',text).lower().split()

"""# STEP 4) FEATURE SELECTION

# STEP 5) DATA PREPROCESSING

X features
"""

vocab_size = 10000
oov_token = '<OOV>'
tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_token)
tokenizer.fit_on_texts(review)

word_index = tokenizer.word_index
print(dict(list(word_index.items())))

review_int = tokenizer.texts_to_sequences(review) # to convert into numbers

# length_review = []
# for i in range(len(review_int)):
#   length_review.append(len(review_int[i]))

# np.median(length_review)

max_len = np.median([len(review_int[i]) for i in range (len(review_int))])

padded_review = pad_sequences(review_int, 
                              maxlen=int(max_len),
                              padding = 'post',
                              truncating='post')

"""y target"""

ohe = OneHotEncoder(sparse=False)
sentiment = ohe.fit_transform(np.expand_dims(sentiment, axis=1))

X_train, X_test, y_train, y_test = train_test_split(padded_review,
                                                    sentiment,
                                                    test_size=0.3,
                                                    random_state=123)

"""MODEL DEVELOPMENT"""

# X_train = np.expand_dims(X_train, axis=-1)
# X_test = np.expand_dims(X_test, axis=-1)

input_shape = np.shape(X_train)[1:]
out_dim = 128

nlp = Sequential()
nlp.add(Input(shape=(input_shape)))
nlp.add(Embedding(vocab_size, out_dim))
nlp.add(Bidirectional(LSTM(128, return_sequences=(True))))
nlp.add(Dropout(0.3))
nlp.add(Bidirectional(LSTM(128)))
nlp.add(Dropout(0.3))
nlp.add(Dense(2, activation='softmax'))
nlp.summary()

plot_model(nlp, show_shapes=(True))

nlp.compile(optimizer='adam',loss='categorical_crossentropy',
              metrics=['accuracy','acc'])

LOGS_PATH = os.path.join(os.getcwd(),'nlp_logs',datetime.datetime.now().
                         strftime('%Y%m%d-%H%M%S'))

tensorboard_callback = TensorBoard(log_dir=LOGS_PATH,histogram_freq=1)

hist = nlp.fit(X_train, y_train, epochs=5,
                 validation_data=(X_test, y_test),
                 callbacks=tensorboard_callback)

# Commented out IPython magic to ensure Python compatibility.
# %load_ext tensorboard

# Commented out IPython magic to ensure Python compatibility.
# %tensorboard --logdir nlp_logs

hist.history.keys() # to show available keys

"""MODEL ANALYSIS"""

from sklearn.metrics import classification_report

y_pred = np.argmax(nlp.predict(X_test), axis=1)
y_actual = np.argmax(y_test, axis=1)

print(classification_report(y_actual, y_pred))

"""MODEL SAVING

*tokenizer*
"""

import json
TOKENIZER_SAVE_PATH = os.path.join(os.getcwd(), 'tokenizer.json')

token_json = tokenizer.to_json()
with open(TOKENIZER_SAVE_PATH, 'w') as file:
  json.dump(token_json, file)

"""*onehotencoder*"""

import pickle
OHE_SAVE_PATH = os.path.join(os.getcwd(), 'ohe.pkl')

with open(OHE_SAVE_PATH, 'wb') as file:
  pickle.dump(ohe, file)

"""*model*"""

NLP_SAVE_PATH = os.path.join(os.getcwd(), 'nlp.h5')
nlp.save(NLP_SAVE_PATH)

"""DEPLOYMENT"""

